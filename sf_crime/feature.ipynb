{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean data\n",
    "train_sf_df = pd.read_csv(filepath_or_buffer='data/train_time_address.csv')\n",
    "test_sf_df = pd.read_csv(filepath_or_buffer='data/test_time_address.csv')\n",
    "train_sf_df.shape, test_sf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sf_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding\n",
    "\n",
    "Extracting time based features via OHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_categories_numericals(df):\n",
    "    \"\"\"Identifying the numerical and categorical columns separately\"\"\"\n",
    "    cols = list(df.columns)\n",
    "    num_cols = list(df._get_numeric_data().columns)\n",
    "    cate_cols = list(set(cols) - set(num_cols))\n",
    "    return cate_cols, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_columns = ['category', 'time', 'address', 'date']\n",
    "\n",
    "def extract_feature_dummies(df, column):\n",
    "    \"\"\"One-Hot-Encoding using Pandas\"\"\"\n",
    "    col_df = df[column]\n",
    "    return pd.get_dummies(data=col_df)\n",
    "\n",
    "def encode_multiple_columns(df, ignore_columns=ignore_columns):\n",
    "    \"\"\"Encoding the multiple columns and vertical stacking them\"\"\"\n",
    "    cate_cols, num_cols = split_categories_numericals(df=df)\n",
    "    \n",
    "    multi_feature_dummies = [df[num_cols]]\n",
    "    for i in cate_cols:\n",
    "        if i not in ignore_columns:\n",
    "            d = extract_feature_dummies(df=df, column=i)\n",
    "            multi_feature_dummies.append(d)\n",
    "\n",
    "    encoded_data = pd.concat(multi_feature_dummies, axis=1)\n",
    "    \n",
    "    return encoded_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Spatial Distance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_pstations_tourists = {\n",
    "    \"sfpd\"                : [37.7725, -122.3894],\n",
    "    \"ingleside\"           : [37.7247, -122.4463],\n",
    "    \"central\"             : [37.7986, -122.4101],\n",
    "    \"northern\"            : [37.7802, -122.4324],\n",
    "    \"mission\"             : [37.7628, -122.4220],\n",
    "    \"tenderloin\"          : [37.7838, -122.4129],\n",
    "    \"taraval\"             : [37.7437, -122.4815],\n",
    "    \"sfpd park\"           : [37.7678, -122.4552],\n",
    "    \"bayview\"             : [37.7298, -122.3977],\n",
    "    \"kma438 sfpd\"         : [37.7725, -122.3894],\n",
    "    \"richmond\"            : [37.7801, -122.4644],\n",
    "    \"police commission\"   : [37.7725, -122.3894],\n",
    "    \"juvenile\"            : [37.7632, -122.4220],\n",
    "    \"southern\"            : [37.6556, -122.4366],\n",
    "    \"sfpd pistol range\"   : [37.7200, -122.4996],\n",
    "    \"sfpd public affairs\" : [37.7754, -122.4039],\n",
    "    \"broadmoor\"           : [37.6927, -122.4748],\n",
    "    #################\n",
    "    \"napa wine country\"      : [38.2975, -122.2869],\n",
    "    \"sonoma wine country\"    : [38.2919, -122.4580],\n",
    "    \"muir woods\"             : [37.8970, -122.5811],\n",
    "    \"golden gate\"            : [37.8199, -122.4783],\n",
    "    \"yosemite national park\" : [37.865101, -119.538330],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpu import haversine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(ij):\n",
    "    \"\"\"Get distance from two coordinates\"\"\"\n",
    "    i = ij[0]\n",
    "    j = ij[1]\n",
    "    distance = haversine_distance(origin=i, destination=j)\n",
    "    return distance\n",
    "\n",
    "def extract_spatial_distance_feature(df, lat_column, lon_column, pname, pcoords):\n",
    "    \"\"\"Compute the distance between pcoords and all the feature values\"\"\"\n",
    "    lat_vals = df[lat_column].to_list()\n",
    "    lon_vals = df[lon_column].to_list()\n",
    "    \n",
    "    df_coords = list(zip(lat_vals, lon_vals))\n",
    "    pcoords_df_coords_combines = zip([pcoords] * len(df), df_coords)\n",
    "    \n",
    "    f = pd.DataFrame()\n",
    "    distances = list(map(get_distance, pcoords_df_coords_combines))\n",
    "    f[pname] = distances\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spatial_distance_multi_features(df, lat_column, lon_column, stations=sf_pstations_tourists):\n",
    "    \"\"\"Compute the spatial distance for multiple features and vertical stacking them\"\"\"\n",
    "    sfeatures = []\n",
    "    \n",
    "    for pname, pcoords in stations.items():\n",
    "        # print(pname, pcoords)\n",
    "        sf = extract_spatial_distance_feature(df, lat_column, lon_column, pname, pcoords)\n",
    "        sfeatures.append(sf)\n",
    "    \n",
    "    spatial_distances = pd.concat(sfeatures, axis=1)\n",
    "    return spatial_distances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features only based on Latitudes and Longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_sum(ll):\n",
    "    \"\"\"Return the sum of lat and lon\"\"\"\n",
    "    lat = ll[0]\n",
    "    lon = ll[1]\n",
    "    return lat + lon\n",
    "\n",
    "def lat_lon_diff(ll):\n",
    "    \"\"\"Return the diff of lat and lon\"\"\"\n",
    "    lat = ll[0]\n",
    "    lon = ll[1]\n",
    "    return lon - lat\n",
    "\n",
    "def lat_lon_sum_square(ll):\n",
    "    \"\"\"Return the square of sum of lat and lon\"\"\"\n",
    "    lat = ll[0]\n",
    "    lon = ll[1]\n",
    "    return (lat + lon) ** 2\n",
    "\n",
    "def lat_lon_diff_square(ll):\n",
    "    \"\"\"Return the square of diff of lat and lon\"\"\"\n",
    "    lat = ll[0]\n",
    "    lon = ll[1]\n",
    "    return (lat - lon) ** 2\n",
    "\n",
    "def lat_lon_sum_sqrt(ll):\n",
    "    \"\"\"Return the sqrt of sum of lat and lon\"\"\"\n",
    "    lat = ll[0]\n",
    "    lon = ll[1]\n",
    "    return (lat**2 + lon**2) ** (1 / 2)\n",
    "\n",
    "def lat_lon_diff_sqrt(ll):\n",
    "    \"\"\"Return the sqrt of diff of lat and lon\"\"\"\n",
    "    lat = ll[0]\n",
    "    lon = ll[1]\n",
    "    return (lon**2 - lat**2) ** (1 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_by_lat_lon(df, lat_column, lon_column):\n",
    "    \"\"\"Compute all lat lon based features\"\"\"\n",
    "    \n",
    "    df_lats = df[lat_column].to_list()\n",
    "    df_lons = df[lon_column].to_list()\n",
    "    ll_zipped = list(zip(df_lats, df_lons))\n",
    "\n",
    "    df_ll = pd.DataFrame()\n",
    "    df_ll['lat_lon_sum'] = list(map(lat_lon_sum, ll_zipped))\n",
    "    df_ll['lat_lon_diff'] = list(map(lat_lon_diff, ll_zipped))\n",
    "    df_ll['lat_lon_sum_square'] = list(map(lat_lon_sum_square, ll_zipped))\n",
    "    df_ll['lat_lon_diff_square'] = list(map(lat_lon_diff_square, ll_zipped))\n",
    "    df_ll['lat_lon_sum_sqrt'] = list(map(lat_lon_sum_sqrt, ll_zipped))\n",
    "    df_ll['lat_lon_diff_sqrt'] = list(map(lat_lon_diff_sqrt, ll_zipped))\n",
    "\n",
    "    return df_ll"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW representation for Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bow_columns = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_vectorizer(df, column, target='category', write_vect=True, kbest=20):\n",
    "    \"\"\"We should only fit on training data to avoid data leakage\"\"\"\n",
    "\n",
    "    model_name = 'vect_bow_{}.pkl'.format(column)\n",
    "    print(model_name)\n",
    "    df_col_val = df[column]\n",
    "\n",
    "    if not os.path.isfile(path='models/' + model_name):\n",
    "        vect = CountVectorizer()\n",
    "        vect.fit(raw_documents=df_col_val)\n",
    "        pickle.dump(vect, open('models/' + model_name, \"wb\"))\n",
    "        \n",
    "    else:\n",
    "        print(\"Model already exists in the directory.\")\n",
    "        vect = pickle.load(open('models/' + model_name, \"rb\"))\n",
    "    \n",
    "    df_col_features = vect.transform(raw_documents=df_col_val)\n",
    "    global best_bow_columns\n",
    "    \n",
    "    if kbest:        \n",
    "        if best_bow_columns.any():\n",
    "            return pd.DataFrame(df_col_features[:, best_bow_columns].toarray(), columns=best_bow_columns)\n",
    "        else:    \n",
    "            fs = SelectKBest(k=kbest)\n",
    "            fs.fit(df_col_features, df[target])\n",
    "            df_col_features = fs.transform(df_col_features)\n",
    "            best_bow_columns = fs.get_support(indices=True)\n",
    "            return pd.DataFrame(df_col_features.toarray(), columns=best_bow_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf representation for Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tfidf_cols = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_vectorizer(df, column, target='category', write_vect=True, kbest=20):\n",
    "    \"\"\"We should only fit on training data to avoid data leakage\"\"\"\n",
    "\n",
    "    model_name = 'vect_tfidf_{}.pkl'.format(column)\n",
    "    print(model_name)\n",
    "    df_col_val = df[column]\n",
    "\n",
    "    if not os.path.isfile(path='models/' + model_name):\n",
    "        vect = TfidfVectorizer()\n",
    "        vect.fit(raw_documents=df_col_val)\n",
    "        pickle.dump(vect, open('models/' + model_name, \"wb\"))\n",
    "    else:\n",
    "        print(\"Model already exists in the directory.\")\n",
    "        vect = pickle.load(open('models/' + model_name, \"rb\"))\n",
    "    \n",
    "    df_col_features = vect.transform(raw_documents=df_col_val)\n",
    "    global best_tfidf_cols\n",
    "\n",
    "    if kbest:\n",
    "        if best_tfidf_cols.any():\n",
    "            return pd.DataFrame(df_col_features[:, best_tfidf_cols].toarray(), columns=best_tfidf_cols)\n",
    "        else:\n",
    "            fs = SelectKBest(k=kbest)\n",
    "            fs.fit(df_col_features, df[target])\n",
    "            df_col_features = fs.transform(df_col_features)\n",
    "            best_tfidf_cols = fs.get_support(indices=True)\n",
    "            return pd.DataFrame(df_col_features.toarray(), columns=best_tfidf_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combing the data\n",
    "\n",
    "* OHE data\n",
    "* Spatial distance features\n",
    "* Spatial latitude and longitude features\n",
    "* Address BoW\n",
    "* Address TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal based features have already been written for both train and test datasets\n",
    "\n",
    "def write_data_features(df, path, write_to_file=True):\n",
    "    encoded_data = encode_multiple_columns(df=df)\n",
    "    sd_features = extract_spatial_distance_multi_features(df=df, lat_column='latitude', lon_column='longitude')\n",
    "    sll_features = features_by_lat_lon(df=df, lat_column='latitude', lon_column='longitude')\n",
    "    address_bow = create_bow_vectorizer(df=df, column='address')\n",
    "    address_tfidf = create_tfidf_vectorizer(df=df, column='address')\n",
    "    sf_df_featurized = pd.concat([encoded_data, sd_features, sll_features, address_bow, address_tfidf], axis=1)\n",
    "\n",
    "    if write_to_file:\n",
    "        sf_df_featurized.to_csv(path_or_buf=path, index=None)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    not os.path.isfile(path='data/train_data_features.csv') and\n",
    "    not os.path.isfile(path='data/test_data_features.csv')\n",
    "   ):\n",
    "    # Training\n",
    "    print(\"Train data\")\n",
    "    write_data_features(df=train_sf_df, path='data/train_data_features.csv')\n",
    "    print('-' * 30)\n",
    "    # Test\n",
    "    print(\"Test data\")\n",
    "    write_data_features(df=test_sf_df, path='data/test_data_features.csv')\n",
    "    print('-' * 30)\n",
    "\n",
    "else:\n",
    "    print(\"Data already exists in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sf_df = pd.read_csv(filepath_or_buffer='data/train_data_features.csv')\n",
    "test_sf_df = pd.read_csv(filepath_or_buffer='data/test_data_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.isfile(path='data/train_data_features_fix.csv')):\n",
    "    # Training Fix\n",
    "    print(\"Train data fix\")\n",
    "    train_sf_df.drop(columns=['Bufano', 'Ferlinghetti'], axis=1, inplace=True)\n",
    "    train_sf_df.to_csv(path_or_buf='data/train_data_features_fix.csv', index=None)\n",
    "\n",
    "else:\n",
    "    print(\"Data already exists in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sf_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
